{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c059c678",
   "metadata": {},
   "source": [
    "### ì›ë³¸ ì°¸ê³  !!!\n",
    "* ì›ë³¸ ê²°ê³¼\n",
    "    - /root/project/data/vision/data/code/yolo/í™ë£¨ëª½/yolo\n",
    "* ì›ë³¸ í•™ìŠµë°ì´í„°\n",
    "    - /root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ\n",
    "* ì›ë³¸ì—ì„œ ë‹¤ì‹œë§Œë“  í•™ìŠµë°ì´í„°\n",
    "    - /root/project/data/vision/data/layout/hong_ru_mong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24483294",
   "metadata": {},
   "source": [
    "# ì¼ë°˜ ëª¨ë¸ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff919b",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fe9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import sys\n",
    "from src.models import yolo_model_load       # ëª¨ë¸ ë³¼ëŸ¬ì˜¨ë‹¤.\n",
    "from src.modules import yolo_model_infer      # ëª¨ë¸ ì¶”ë¡ í•œë‹¤.\n",
    "from src.utils import image_bbox_show_YOLO, select_highest_confidence_bbox, resize_bbox\n",
    "import time\n",
    "\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "\n",
    "\n",
    "model_path = '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "model = yolo_model_load(model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ad654",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_path = test_paths[1]\n",
    "\n",
    "img1 = cv2.imread(img_path)\n",
    "print(f'before: {img1.shape}')\n",
    "img2 = cv2.resize(img1, dsize=None, fx=0.25, fy=0.25)\n",
    "print(f'after: {img2.shape}')\n",
    "device = 'cuda:2'\n",
    "# img_path=cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "start = time.time()\n",
    "prediction = yolo_model_infer(model, img_path=img_path, device=device, conf=0.3)\n",
    "prediction_result = select_highest_confidence_bbox(prediction)\n",
    "resize_bbox_result = resize_bbox(prediction_result)     # [(classes, n_boxes, confs), .... ]\n",
    "final = time.time()\n",
    "print(f'ì¶”ë¡  ì¢…ë£Œ: {final-start:.3f}s')\n",
    "image_bbox_show_YOLO(img=img_path, \n",
    "                    bbox=resize_bbox_result, \n",
    "                    fontsize=10, n_classes = 3, boxline_thickness=8, \n",
    "                    img_x_size=20, img_y_size=20)\n",
    "\"\"\"\n",
    "classes\n",
    "0: G1\n",
    "1: G2\n",
    "2: G3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88683a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af393725",
   "metadata": {},
   "source": [
    "# TensorRT ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f81719",
   "metadata": {},
   "source": [
    "### format - saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3455a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.187 ğŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:2 (NVIDIA L40S, 45589MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.pt' with input shape (1, 3, 1120, 1120) BCHW and output shape(s) (1, 7, 25725) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.82...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.1s, saved as '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.onnx' (10.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.7.0.post1...\n",
      "[01/06/2026-08:37:54] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -2274, GPU +430, now: CPU 3449, GPU 1780 (MiB)\n",
      "[01/06/2026-08:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "[01/06/2026-08:37:54] [TRT] [I] Input filename:   /root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.onnx\n",
      "[01/06/2026-08:37:54] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[01/06/2026-08:37:54] [TRT] [I] Opset version:    19\n",
      "[01/06/2026-08:37:54] [TRT] [I] Producer name:    pytorch\n",
      "[01/06/2026-08:37:54] [TRT] [I] Producer version: 2.6.0\n",
      "[01/06/2026-08:37:54] [TRT] [I] Domain:           \n",
      "[01/06/2026-08:37:54] [TRT] [I] Model version:    0\n",
      "[01/06/2026-08:37:54] [TRT] [I] Doc string:       \n",
      "[01/06/2026-08:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 1120, 1120) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 7, 25725) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as /root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine\n",
      "[01/06/2026-08:37:54] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/06/2026-08:38:11] [TRT] [I] Compiler backend is used during engine build.\n",
      "[01/06/2026-08:38:42] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Host Persistent Memory: 547744 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Max Scratch Memory: 24951296 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 242 steps to complete.\n",
      "[01/06/2026-08:38:43] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.101ms to assign 11 blocks to 242 nodes requiring 63838720 bytes.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Activation Memory: 63837696 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Weights Memory: 11123960 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Engine generation completed in 48.9023 seconds.\n",
      "[01/06/2026-08:38:43] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 412 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 52.8s, saved as '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine' (14.7 MB)\n",
      "\n",
      "Export complete (53.2s)\n",
      "Results saved to \u001b[1m/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine imgsz=1120  \n",
      "Validate:        yolo val task=detect model=/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine imgsz=1120 data=/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "\n",
    "\n",
    "infer_checkpoints = '/root/project/data/vision/data/layout/hong_ru_mong/checkpoints/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "infer_checkpoints = '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "model = YOLO(infer_checkpoints)\n",
    "# Export the model to TensorRT format\n",
    "model.export(format='engine', device='cuda:2')     \n",
    "# Load the exported TensorRT model\n",
    "# trt_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67fc31",
   "metadata": {},
   "source": [
    "### Load the exported TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4b71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ: 0.031s\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import sys\n",
    "sys.path.append('/root/project/layout_parsing/í™ë£¨ëª½/yolo')\n",
    "from src.models import yolo_model_load       # ëª¨ë¸ ë³¼ëŸ¬ì˜¨ë‹¤.\n",
    "from src.modules import yolo_model_infer      # ëª¨ë¸ ì¶”ë¡ í•œë‹¤.\n",
    "from src.utils import image_bbox_show_YOLO, select_highest_confidence_bbox, resize_bbox\n",
    "import time\n",
    "\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "device = 'cuda:3'\n",
    "infer_checkpoints = '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "\n",
    "start = time.time()\n",
    "trt_model = yolo_model_load(infer_checkpoints)\n",
    "final = time.time()\n",
    "print(f'ëª¨ë¸ ë¡œë“œ: {final-start:.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76ed05",
   "metadata": {},
   "source": [
    "### ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = test_paths[1]\n",
    "img1 = cv2.imread(img_path)\n",
    "print(f'before: {img1.shape}')\n",
    "img2 = cv2.resize(img1, dsize=None, fx=0.3, fy=0.3)\n",
    "print(f'after: {img2.shape}')\n",
    "\n",
    "img_path=img2\n",
    "\n",
    "start = time.time()\n",
    "trt_predition = trt_model.predict(img_path, conf=0.3, device=device)[0]\n",
    "print('TrT prediction')\n",
    "\n",
    "trt_prediction_result = select_highest_confidence_bbox(trt_predition)\n",
    "trt_resize_bbox_result = resize_bbox(trt_prediction_result)\n",
    "final = time.time()\n",
    "print(f'ì¶”ë¡  ì¢…ë£Œ: {final-start:.3f}s')\n",
    "image_bbox_show_YOLO(img=img_path, \n",
    "                    bbox=trt_resize_bbox_result, \n",
    "                    fontsize=10, n_classes = 3, boxline_thickness=8, \n",
    "                    img_x_size=20, img_y_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".champ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

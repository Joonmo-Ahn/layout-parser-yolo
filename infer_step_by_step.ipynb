{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c059c678",
   "metadata": {},
   "source": [
    "### ë‹¤ìŒ ì›ë³¸ ê²°ê³¼ ì°¸ê³ \n",
    "/root/project/data/vision/data/code/yolo/í™ë£¨ëª½/yolo\n",
    "### ì›ë³¸ í•™ìŠµ ë°ì´í„°\n",
    "/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ\n",
    "### ë‹¤ì‹œ ë§Œë“  í•™ìŠµë°ì´í„°\n",
    "/root/project/data/vision/data/layout/hong_ru_mong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d4604",
   "metadata": {},
   "source": [
    "### ì›ë³¸ ì°¸ê³  !!!\n",
    "* ì›ë³¸ ê²°ê³¼\n",
    "    - /root/project/data/vision/data/code/yolo/í™ë£¨ëª½/yolo\n",
    "* ì›ë³¸ í•™ìŠµë°ì´í„°\n",
    "    - /root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ\n",
    "* ì›ë³¸ì—ì„œ ë‹¤ì‹œë§Œë“  í•™ìŠµë°ì´í„°\n",
    "    - /root/project/data/vision/data/layout/hong_ru_mong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24483294",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° - ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b193ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def xml_to_yolo_bbox(bbox, w, h):\n",
    "    #  bbox = [xmin, ymin, xmax, ymax]\n",
    "    #  bbox = [x1, y1, x2, y2]\n",
    "    x_center = ((bbox[2]+bbox[0])/2)/w\n",
    "    y_center = ((bbox[3]+bbox[1])/2)/h\n",
    "    width = ((bbox[2]-bbox[0])/2)/w\n",
    "    height = ((bbox[3]-bbox[1])/2)/h\n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "def yolo_to_xml_bbox(bbox, w, h):\n",
    "    #  bbox = [x_center, y_center, width, height]\n",
    "    x_min = (bbox[0]-bbox[2]) * w\n",
    "    y_min = (bbox[1]-bbox[3]) * h\n",
    "    x_max = (bbox[0]+bbox[2]) * w\n",
    "    y_max = (bbox[1]+bbox[3]) * h\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "\n",
    "\n",
    "infer_checkpoints = '/root/project/data/vision/data/layout/hong_ru_mong/checkpoints/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "model11 = YOLO(infer_checkpoints)\n",
    "\n",
    "\n",
    "img = cv2.imread(test_paths[1])\n",
    "prediction = model11.predict(img, conf=0.3)[0]\n",
    "\n",
    "prediction0 = prediction.plot(line_width=12, font_size = 2)\n",
    "prediction1 = prediction0[:, :, ::-1]\n",
    "prediction2 = Image.fromarray(prediction1)\n",
    "prediction2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae31b2e",
   "metadata": {},
   "source": [
    "# ì¤‘ë³µë°•ìŠ¤ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7a5a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ì²© ì œê±°: 3ê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "37 34\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if len(prediction) > 1:\n",
    "    indices_to_remove = set()\n",
    "    bboxes = prediction.boxes.xyxy #[p.boxes.xyxy.cpu() for p in prediction]\n",
    "    confs = prediction.boxes.conf\n",
    "\n",
    "    for i in range(len(bboxes)):\n",
    "        for j in range(i + 1, len(bboxes)):\n",
    "            if i in indices_to_remove or j in indices_to_remove:\n",
    "                continue\n",
    "\n",
    "            conf_i = confs[i]\n",
    "            conf_k = confs[j]\n",
    "            \n",
    "            box_i = bboxes[i]\n",
    "            [x1_i, y1_i, x2_i, y2_i] = box_i\n",
    "            center_x_i = (x1_i + x2_i) / 2\n",
    "            center_y_i = (y1_i + y2_i) / 2\n",
    "\n",
    "            box_j = bboxes[j]\n",
    "            [x1_j, y1_j, x2_j, y2_j] = box_j\n",
    "            center_x_j = (x1_j + x2_j) / 2\n",
    "            center_y_j = (y1_j + y2_j) / 2\n",
    "\n",
    "            is_center_i_in_j = (x1_j < center_x_i < x2_j) and (y1_j < center_y_i < y2_j)\n",
    "            is_center_j_in_i = (x1_i < center_x_j < x2_i) and (y1_i < center_y_j < y2_i)\n",
    "\n",
    "            if is_center_i_in_j or is_center_j_in_i:\n",
    "                # ì‹ ë¢°ë„ê°€ ë‚®ì€ ë°•ìŠ¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì œê±° ëª©ë¡ì— ì¶”ê°€\n",
    "                if confs[i] < confs[j]:\n",
    "                    indices_to_remove.add(i)\n",
    "                else:\n",
    "                    indices_to_remove.add(j)\n",
    "if indices_to_remove:\n",
    "        # torch.bool íƒ€ìž…ì˜ ë§ˆìŠ¤í¬ ìƒì„±\n",
    "    keep_mask = torch.ones(len(bboxes), dtype=torch.bool)\n",
    "    for idx in indices_to_remove:\n",
    "        keep_mask[idx] = False\n",
    "    prediction_result = prediction[keep_mask]\n",
    "    print(f\"ì¤‘ì²© ì œê±°: {len(indices_to_remove)}ê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(len(prediction), len(prediction_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ceacb6",
   "metadata": {},
   "source": [
    "# ë°•ìŠ¤ í¬ê¸° ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe97bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 2 # 20% í™•ëŒ€\n",
    "r = prediction_result\n",
    "# result.boxesê°€ ë¹„ì–´ìžˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í¬ê¸° ì¡°ì ˆ ì‹¤í–‰\n",
    "if len(r) > 0:\n",
    "    modified_boxes_xyxy = r.boxes.xyxy.clone()\n",
    "    confs = r.boxes.conf\n",
    "    classes = r.boxes.cls\n",
    "    n_boxes = []\n",
    "\n",
    "    for i, box in enumerate(modified_boxes_xyxy):\n",
    "        x1, y1, x2, y2 = box\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        new_width = width * scale_factor\n",
    "        new_height = height * scale_factor\n",
    "\n",
    "        new_x1 = center_x - new_width / 2\n",
    "        new_y1 = center_y - new_height / 2\n",
    "        new_x2 = center_x + new_width / 2\n",
    "        new_y2 = center_y + new_height / 2\n",
    "\n",
    "        img_height, img_width = r.orig_shape\n",
    "        new_x1 = max(0, new_x1).cpu().item()\n",
    "        new_y1 = max(0, new_y1).cpu().item()\n",
    "        new_x2 = min(img_width, new_x2).cpu().item()\n",
    "        new_y2 = min(img_height, new_y2).cpu().item()\n",
    "\n",
    "        # modified_boxes_xyxy[i] = torch.tensor([new_x1, new_y1, new_x2, new_y2])\n",
    "        n_boxes.append([int(new_x1), int(new_y1), int(new_x2), int(new_y2)])\n",
    "\n",
    "    # result ê°ì²´ì˜ bbox ì •ë³´ë¥¼ ìˆ˜ì •ëœ ì •ë³´ë¡œ êµì²´\n",
    "new_result_bboxes = [[int(n[0].item()), n[1], n[2].item()] for n in zip(classes, n_boxes, confs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c66683",
   "metadata": {},
   "source": [
    "# ìµœì¢… ê²°ê³¼ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.cm as cm    # Colormapì„ ìœ„í•´ ì¶”ê°€\n",
    "\n",
    "def image_bbox_show_YOLO(img, bbox, fontsize, n_classes = 3, boxline_thickness=2, img_x_size=10, img_y_size=16):\n",
    "    \"\"\"\n",
    "    bbox :\n",
    "        [\n",
    "            [[0, [3304, 634, 3422, 1284], 0.9730206727981567],\n",
    "            [0, [5449, 665, 5571, 1221], 0.960870087146759],\n",
    "            [0, [1883, 692, 1989, 1554], 0.909072756767273],\n",
    "            [0, [1486, 708, 1622, 1815], 0.9034684896469116],\n",
    "            [0, [801, 695, 924, 1734], 0.8977305293083191],\n",
    "            [2, [4133, 2735, 6292, 5228], 0.8887861371040344],\n",
    "            [1, [2776, 664, 2905, 1669], 0.8649317622184753],\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # ê° class_numberì— ë”°ë¥¸ ìƒ‰ìƒ ë§¤í•‘ ì •ì˜\n",
    "    # ì—¬ê¸°ì— ì›í•˜ëŠ” í´ëž˜ìŠ¤ ë²ˆí˜¸ì™€ RGB ìƒ‰ìƒ íŠœí”Œì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "    # ì˜ˆì‹œ: {í´ëž˜ìŠ¤_ë²ˆí˜¸: (R, G, B)}\n",
    "    # class_colors = {\n",
    "    #     0: {\"box\": (255, 0, 0), \"text\": (255, 255, 0)},  # í´ëž˜ìŠ¤ 0: ë¹¨ê°„ìƒ‰ ë°•ìŠ¤, ë…¸ëž€ìƒ‰ í…ìŠ¤íŠ¸\n",
    "    #     1: {\"box\": (0, 255, 0), \"text\": (0, 0, 255)},  # í´ëž˜ìŠ¤ 1: ì´ˆë¡ìƒ‰ ë°•ìŠ¤, íŒŒëž€ìƒ‰ í…ìŠ¤íŠ¸\n",
    "    #     2: {\"box\": (0, 0, 255), \"text\": (255, 0, 255)},  # í´ëž˜ìŠ¤ 2: íŒŒëž€ìƒ‰ ë°•ìŠ¤, ë§ˆì  íƒ€ í…ìŠ¤íŠ¸\n",
    "    #     3: {\"box\": (255, 255, 0), \"text\": (0, 0, 0)},    # í´ëž˜ìŠ¤ 3: ë…¸ëž€ìƒ‰ ë°•ìŠ¤, ê²€ì€ìƒ‰ í…ìŠ¤íŠ¸\n",
    "    #     # ë” ë§Žì€ í´ëž˜ìŠ¤ì— ëŒ€í•œ ìƒ‰ìƒì„ ì¶”ê°€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "    #     # ê¸°ë³¸ ìƒ‰ìƒ (ë§Œì•½ ì •ì˜ë˜ì§€ ì•Šì€ í´ëž˜ìŠ¤ ë²ˆí˜¸ê°€ ë“¤ì–´ì˜¬ ê²½ìš°)\n",
    "    #     \"default\": {\"box\": (128, 128, 128), \"text\": (255, 255, 255)} # íšŒìƒ‰ ë°•ìŠ¤, í°ìƒ‰ í…ìŠ¤íŠ¸\n",
    "    # }\n",
    "\n",
    "    color_map = cm.get_cmap('hsv', max(n_classes, 1))\n",
    "\n",
    "    if isinstance(img, str):\n",
    "        img = cv2.imread(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCVëŠ” BGR, Matplotlibì€ RGBì´ë¯€ë¡œ ë³€í™˜\n",
    "\n",
    "    fontpath = '/root/project/data_3090/jmahn/HANBatang.ttf'\n",
    "    fontposition = 0.6 # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì¡°ì •ì„ ìœ„í•œ ë¹„ìœ¨\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ PIL Imageë¡œ ë³€í™˜\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    for m in tqdm(bbox, total=len(bbox)):\n",
    "        class_number = m[0]\n",
    "        z = m[1]\n",
    "        # p1, p2, p3, p4ëŠ” x1, y1, x2, y2ë¥¼ ì˜ë¯¸\n",
    "        x1, y1, x2, y2 = int(z[0]), int(z[1]), int(z[2]), int(z[3])\n",
    "        prob = m[2] # í™•ë¥  (ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "\n",
    "        text = str(class_number) # ë°•ìŠ¤ ìœ„ì— í‘œì‹œí•  í…ìŠ¤íŠ¸ (í´ëž˜ìŠ¤ ë²ˆí˜¸)\n",
    "        # class_numberì— ë”°ë¼ ìƒ‰ìƒ ì¶”ì¶œ\n",
    "        # Colormapì—ì„œ (R, G, B, A) ê°’ì„ ì–»ê³ , 0-255 ë²”ìœ„ë¡œ ë³€í™˜\n",
    "        # n_classesê°€ 0ì´ ì•„ë‹ ë•Œë§Œ ì •ê·œí™”\n",
    "\n",
    "        normalized_class_number = class_number / (n_classes -1) if n_classes > 1 else 0   # 0 <= normalized_class_number <= 1\n",
    "        rgba_color = color_map(normalized_class_number)\n",
    "        select_color = (int(rgba_color[0]*255), int(rgba_color[1]*255), int(rgba_color[2]*255))\n",
    "        # í°íŠ¸ í¬ê¸° ê³„ì‚°\n",
    "        # ë°•ìŠ¤ ë„ˆë¹„ì— ë¹„ë¡€í•˜ì—¬ í°íŠ¸ í¬ê¸° ì¡°ì ˆ\n",
    "        font_size = max(int((x2 - x1) * fontsize), 10) # ìµœì†Œ í°íŠ¸ í¬ê¸° 10\n",
    "        font = ImageFont.truetype(fontpath, font_size)\n",
    "\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì¡°ì • (ë°•ìŠ¤ ìœ„ì— í‘œì‹œ)\n",
    "        # í…ìŠ¤íŠ¸ì˜ ë†’ì´ë¥¼ ê³ ë ¤í•˜ì—¬ y1ì—ì„œ í…ìŠ¤íŠ¸ í¬ê¸°ë§Œí¼ ìœ„ë¡œ ì´ë™\n",
    "        # text_width, text_height = draw.textsize(text, font=font)\n",
    "        # text_position = (x1, y1 - text_height - 5) # 5ëŠ” ì—¬ë°±\n",
    "\n",
    "        # ì´ë¯¸ì§€ ê²½ê³„ ì²´í¬: í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ì§€ ìœ„ë¡œ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ ì¡°ì •\n",
    "        # if text_position[1] < 0:\n",
    "        #     text_position = (x1, y1 + 5) # ë§Œì•½ ìœ„ë¡œ ë„˜ì–´ê°€ë©´ ë°•ìŠ¤ ì•„ëž˜ì— í‘œì‹œ\n",
    "\n",
    "        # draw.text(text_position, text, font=font, fill=select_color)\n",
    "\n",
    "        # ë°•ìŠ¤ ê·¸ë¦¬ê¸° (PIL Imageë¥¼ ë‹¤ì‹œ NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ OpenCV ì‚¬ìš©)\n",
    "        # OpenCVëŠ” RGB ìˆœì„œë¡œ ìƒ‰ìƒì„ ë°›ìœ¼ë¯€ë¡œ, PILì—ì„œ ì„¤ì •ëœ ìƒ‰ìƒ íŠœí”Œì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\n",
    "        # draw.rectangleì€ PIL ImageDrawì˜ ë©”ì„œë“œì´ë¯€ë¡œ PIL Imageì— ì§ì ‘ ê·¸ë¦¼\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=select_color, width=boxline_thickness)\n",
    "\n",
    "    # ëª¨ë“  ë“œë¡œìž‰ì´ ëë‚œ í›„ NumPy ë°°ì—´ë¡œ ìµœì¢… ë³€í™˜\n",
    "    imgs = np.array(image)\n",
    "\n",
    "    plt.figure(figsize=(img_x_size, img_y_size))\n",
    "    print(imgs.shape)\n",
    "    plt.imshow(imgs)\n",
    "    plt.axis('off') # ì¶• ì •ë³´ ì œê±°\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_bbox_show_YOLO(img=img, \n",
    "                     bbox=new_result_bboxes, \n",
    "                     fontsize=10, n_classes = 3, boxline_thickness=8, \n",
    "                     img_x_size=20, img_y_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af393725",
   "metadata": {},
   "source": [
    "# TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f81719",
   "metadata": {},
   "source": [
    "### format - saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3455a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.187 ðŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:2 (NVIDIA L40S, 45589MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,737 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.pt' with input shape (1, 3, 1120, 1120) BCHW and output shape(s) (1, 7, 25725) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.82...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.1s, saved as '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.onnx' (10.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.7.0.post1...\n",
      "[01/06/2026-08:37:54] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -2274, GPU +430, now: CPU 3449, GPU 1780 (MiB)\n",
      "[01/06/2026-08:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "[01/06/2026-08:37:54] [TRT] [I] Input filename:   /root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.onnx\n",
      "[01/06/2026-08:37:54] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[01/06/2026-08:37:54] [TRT] [I] Opset version:    19\n",
      "[01/06/2026-08:37:54] [TRT] [I] Producer name:    pytorch\n",
      "[01/06/2026-08:37:54] [TRT] [I] Producer version: 2.6.0\n",
      "[01/06/2026-08:37:54] [TRT] [I] Domain:           \n",
      "[01/06/2026-08:37:54] [TRT] [I] Model version:    0\n",
      "[01/06/2026-08:37:54] [TRT] [I] Doc string:       \n",
      "[01/06/2026-08:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 1120, 1120) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 7, 25725) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as /root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine\n",
      "[01/06/2026-08:37:54] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/06/2026-08:38:11] [TRT] [I] Compiler backend is used during engine build.\n",
      "[01/06/2026-08:38:42] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Host Persistent Memory: 547744 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Max Scratch Memory: 24951296 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 242 steps to complete.\n",
      "[01/06/2026-08:38:43] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.101ms to assign 11 blocks to 242 nodes requiring 63838720 bytes.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Activation Memory: 63837696 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Total Weights Memory: 11123960 bytes\n",
      "[01/06/2026-08:38:43] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[01/06/2026-08:38:43] [TRT] [I] Engine generation completed in 48.9023 seconds.\n",
      "[01/06/2026-08:38:43] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 412 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 52.8s, saved as '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine' (14.7 MB)\n",
      "\n",
      "Export complete (53.2s)\n",
      "Results saved to \u001b[1m/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine imgsz=1120  \n",
      "Validate:        yolo val task=detect model=/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine imgsz=1120 data=/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/imgsz_1120_batch_4_epoch_20/weights/best.engine'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "\n",
    "\n",
    "infer_checkpoints = '/root/project/data/vision/data/layout/hong_ru_mong/checkpoints/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "infer_checkpoints = '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "model = YOLO(infer_checkpoints)\n",
    "# Export the model to TensorRT format\n",
    "model.export(format='engine', device='cuda:2')     \n",
    "# Load the exported TensorRT model\n",
    "# trt_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67fc31",
   "metadata": {},
   "source": [
    "### Load the exported TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import sys\n",
    "sys.path.append('/root/project/layout_parsing/í™ë£¨ëª½/yolo')\n",
    "from src.models import yolo_model_load       # ëª¨ë¸ ë³¼ëŸ¬ì˜¨ë‹¤.\n",
    "from src.modules import yolo_model_infer      # ëª¨ë¸ ì¶”ë¡ í•œë‹¤.\n",
    "from src.utils import image_bbox_show_YOLO, select_highest_confidence_bbox, resize_bbox\n",
    "import time\n",
    "\n",
    "\n",
    "test_paths = ['/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0031.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_006_ç´…æ¨“å¤¢_í™ë£¨ëª½(49)_0006.jpg',\n",
    "      '/root/project/data/vision/data/nara/í™ë£¨ëª½/í™ë£¨ëª½1-6ê¶Œ/layout/images/K4-6864_001_ç´…æ¨“å¤¢_í™ë£¨ëª½(54)_(1)_0029.jpg']\n",
    "device = 'cuda:3'\n",
    "infer_checkpoints = '/root/project/layout_parsing/í™ë£¨ëª½/yolo/train/test/img_resize_1120_batch_4_epoch_202/weights/best.pt'\n",
    "\n",
    "\n",
    "\n",
    "img_path = test_paths[1]\n",
    "\n",
    "start = time.time()\n",
    "trt_model = yolo_model_load(infer_checkpoints)\n",
    "final = time.time()\n",
    "print(f'ëª¨ë¸ ë¡œë“œ: {final-start:.3f}s')\n",
    "\n",
    "start = time.time()\n",
    "trt_predition = trt_model.predict(img_path, conf=0.3, device=device)[0]\n",
    "print('TrT prediction')\n",
    "\n",
    "trt_prediction_result = select_highest_confidence_bbox(trt_predition)\n",
    "trt_resize_bbox_result = resize_bbox(trt_prediction_result)\n",
    "final = time.time()\n",
    "print(f'ì¶”ë¡  ì¢…ë£Œ: {final-start:.3f}s')\n",
    "image_bbox_show_YOLO(img=img_path, \n",
    "                    bbox=trt_resize_bbox_result, \n",
    "                    fontsize=10, n_classes = 3, boxline_thickness=8, \n",
    "                    img_x_size=20, img_y_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".champ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
